[{"authors":null,"categories":null,"content":"This is the first post in a series. Where I want to highlight my all time favorite books. These are technical books that helped me throughout my studies and career. Most of them are very easy to read and translate a lot of knowledge to you very quickly.\nI start here with Hands-On Machine Learning by Aurélien Géron. This book actually made me want to start this series. As I really want to recommend this book to anyone interested in the topic. I started reading the first edition in 2017. And from my experience it covers everything you need to know in the typical machine learning interview. At the same time it also serves as a great reference for your actual work.\nThe idea of the book is to give a very good explanation of a concept and at the same time show actual python code. As a side note. This shows how great python is. The author also offers the code as jupyter notebooks so that you can really play with actual data and code. Also this appears as the key selling point that is not what make the book great. To be honest, everything is so well explained that you barely need this option.\nThe book starts with overview of machine learning on just 30 pages and then describes a typical project with around 50 pages. Wow! The amazing thing is that I do not think the author misses anything important. He continues with describing classic machine learning in scikit learn. Followed by neuronal networks and deep learning in Keras and Tensorflow 2.0. He introduces concepts as needed. Where others follow a chronological approach and tend to write a history book of machine learning Aurélien Géron focuses on utility and logic for the reader. Also he covers recent research results including references. So if you want to go deeper on a specific approach you can read the original paper.\nOverall, If you want to apply ML and just have money for one book. Buy this one. However, if your focus is on currently more niche aspects of machine learning like unsupervised or bayesian approaches. This book might be not for you. For bayesian approaches there is a great book by David Barber called Bayesian Reasoning and Machine Learning. For unsupervised ML I currently cannot recommend any book. However, I think for more than 90% of people \u0026lsquo;Hands-on Machine Learning\u0026rsquo; will be the right book which covers everything they need for their daily ML problems.\n","date":1583056800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583056800,"objectID":"b25f99205301dbaec71095e6e3c1c4d7","permalink":"http://AlexAndSchaefer.github.io/post/handson/","publishdate":"2020-03-01T10:00:00Z","relpermalink":"/post/handson/","section":"post","summary":"This is the first post in a series. Where I want to highlight my all time favorite books. These are technical books that helped me throughout my studies and career. Most of them are very easy to read and translate a lot of knowledge to you very quickly.\nI start here with Hands-On Machine Learning by Aurélien Géron. This book actually made me want to start this series. As I really want to recommend this book to anyone interested in the topic.","tags":[],"title":"Book Review - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow","type":"post"},{"authors":["Ru Kong, Jingwei Li, Csaba Orban, Mert Rory Sabuncu, Hesheng Liu, Alexander Schaefer, Nanbo Sun, Xi-Nian Zuo, Avram Holmes, Simon B. Eickhoff, B.T. Thomas Yeo"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1527206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527206400,"objectID":"f45825de1fc884fec5a204e0c2359676","permalink":"http://AlexAndSchaefer.github.io/publication/kong/","publishdate":"2018-05-25T00:00:00Z","relpermalink":"/publication/kong/","section":"publication","summary":"Resting-state functional magnetic resonance imaging (rs-fMRI) offers the opportunity to delineate individual-specific brain networks. A major question is whether individual-specific network topography (i.e., location and spatial arrangement) is behaviorally relevant. Here, we propose a multi-session hierarchical Bayesian model (MS-HBM) for estimating individual-specific cortical networks and investigate whether individual-specific network topography can predict human behavior. The multiple layers of the MS-HBM explicitly differentiate intra-subject (within-subject) from inter-subject (between-subject) network variability. By ignoring intra-subject variability, previous network mappings might confuse intra-subject variability for inter-subject differences. Compared with other approaches, MS-HBM parcellations generalized better to new rs-fMRI and task-fMRI data from the same subjects. More specifically, MS-HBM parcellations estimated from a single rs-fMRI session (10 minutes) showed comparable generalizability as parcellations estimated by two state-of-the-art methods using five sessions (50 minutes). We also showed that behavioral phenotypes across cognition, personality and emotion could be predicted by individual-specific network topography with modest accuracy, comparable to previous reports predicting phenotypes based on connectivity strength. Network topography estimated by MS-HBM was more effective for behavioral prediction than network size, as well as network topography estimated by other parcellation approaches. Thus, similar to connectivity strength, individual-specific network topography might also serve as a fingerprint of human behavior.","tags":null,"title":"Spatial Topography of Individual-Specific Cortical Networks Predicts Human Cognition, Personality and Emotion","type":"publication"},{"authors":null,"categories":null,"content":" In my work as a data scientist at Project A. I often encounter the task of predicting customer behavior. For many businesses it can be very valuable to estimate which one of their new customers will be the most profitable in the future. Or which one of the existing customers will stop being valuable. In this context many tasks such as predicting churn, customer lifetime value or ending of newsletter subscription are of interest. While these tasks sound very different the principles can be easily generalized to any predictive modeling task. Here, I will evaluate on the general principle.\nTime scale Usually, it is not of interest to predict any arbitrary time point in the future. It is sufficient to estimate the target variable of interest for a specific point in time, e.g. the behavior of a customer in 12 months from now. This makes the modeling of our problem a lot easier. Instead of forecasting a complex time series we only need to make a single point estimate.\nTo formalize this a little bit. We want to predict the customer behavior in $t$ months from now. And denote the customer behavior as $y_t$ and our prediction with $\\hat{y_t}$. We do this prediction based on our current observations of the customer $x_0$ (e.g. his buying frequency, his created revenue up to date) .\nData Data is the key component in predictive modeling. We want to make a prediction of the future based on our observations in the past. So in order to train a predictive model that predicts how customers behave in $t$ months from now. We actually go $t$ months in to the past look at our observation $x_{-t}$ and build a model $m_{now}$ that predicts our current customer behavior $y_{now}$ $$ \\underset{m_{now}}{\\mathrm{argmin}} (\\hat{y}_{now} - y_{now}) \\underset{m_{now}}{\\mathrm{argmin}} (f(x_{-t},m_{now}) - y_{now}). $$ We do also add a regularization term $R(f)$ as our aim is not learning the training data perfectly but to generalize as good as possible $$ \\underset{m_{now}}{\\mathrm{argmin}} \\bigg(\\Big(f(x_{-t},m_{now}) - y_{now}\\Big) - \\lambda \\Big(R(f)\\Big)\\bigg). $$\nValidation The classical way of validating machine learning models is cross validation. For cross validation we estimate or train our model on a subset of the data. And estimate its performance on the remaining set. We can do this multiple time so that each customer was at some point part of the training dataset and at some point part of the testing dataset. This is good because it tells us how well we generalize towards new customers.\nHowever, here we are not so much interested in how well our predictive model generalizes to new customers. Rather we are interested how well our model generalizes over time. Therefore we might want to validate our approach by learning a model $m_{-t}$ that trains on observation data $x_{-2t}$: $$ \\underset{m_{-t}}{\\mathrm{argmin}} \\bigg(\\Big(f(x_{-2t},m_{-t}) - y_{-t}\\Big) - \\lambda \\Big(R(f)\\Big)\\bigg). $$ This model is ideal to validate the temporal generalization of our approach. We can apply it onto our observation at time point $-t$ to predict our current observation: $$ f(x_{-t},m_{-t}) = \\hat{y}_{now} . $$ The difference between our prediction and the actual customer behavior: $$ {y}_{now} - \\hat{y}_{now} , $$ can be a good estimate how our approach generalizes over time.\nAnother valid approach is store your predictions and to monitor their development and ideally their convergence over time.\nCommunication and Alignment A problem which is a lot of times underestimated is intense communication with the stakeholders. Often additional hours spend in the initial alignment phase can save days in later phases. Here, I recommend to ask stakeholders how they want to use the result of the predictive model. Sometimes, you will find out that a simple statistic is sufficient or the problem is very different to how it was initially described. Additionally, I recommend to mutually align on points and iterate until everyone is on the same page.\nMethods On the contrary a problem which is often over estimated is the choice of a machine learning method. Often people want to work a like to machine learning method papers and compare 5 to 10 different methods. While a new machine learning method has to be compared to others to give evidence for its relevance the value in creating a new data product is rather small. Usually spending time in feature engineering or acquiring more labeled data is much better spend. Here is an Illustration by Banko and Brill showing impressively that data is more powerful than algorithms:\nIt is claimed that more modern approaches like deep learning are mainly performing better because they able to encode more data. This also points to algorithmic complexity and the question how well an algorithms scales with data.\n","date":1519293600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519293600,"objectID":"11219b5dd1abd6fcc41d7c11f29899d8","permalink":"http://AlexAndSchaefer.github.io/post/predictingcustomerbehaviour/","publishdate":"2018-02-22T10:00:00Z","relpermalink":"/post/predictingcustomerbehaviour/","section":"post","summary":"In my work as a data scientist at Project A. I often encounter the task of predicting customer behavior. For many businesses it can be very valuable to estimate which one of their new customers will be the most profitable in the future. Or which one of the existing customers will stop being valuable. In this context many tasks such as predicting churn, customer lifetime value or ending of newsletter subscription are of interest.","tags":[],"title":"Predicting Customer Behavior - A General Approach","type":"post"},{"authors":null,"categories":null,"content":"","date":1507852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507852800,"objectID":"4512354f70d3243b649bd725cd6d0dfd","permalink":"http://AlexAndSchaefer.github.io/talk/techday/","publishdate":"2017-10-13T00:00:00Z","relpermalink":"/talk/techday/","section":"talk","summary":"Predicting customer behavior can be a key insight to steer marketing and sales activities. I will present a case study explaining prerequisites, methodology as well as common problems and pitfalls.","tags":null,"title":"Predicting customer behaviour with gradient boosting","type":"talk"},{"authors":null,"categories":null,"content":"","date":1507680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507680000,"objectID":"e12b0c50534bbdd5ce5b993c94e32714","permalink":"http://AlexAndSchaefer.github.io/talk/onproduct-meetup/","publishdate":"2017-10-11T00:00:00Z","relpermalink":"/talk/onproduct-meetup/","section":"talk","summary":"In my talk I want to give a short introduction to machine learning and highlight differences between machine learning projects and traditional software projects.","tags":null,"title":"Getting started with Machine Learning","type":"talk"},{"authors":["Alexander Schaefer, Ru Kong, Evan M. Gordon, Timothy O. Laumann, Xi-Nian Zuo, Avram Holmes, Simon B. Eickhoff, B. T. Thomas Yeo"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1496707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496707200,"objectID":"d191c5ff4241b974c8952a6051ede121","permalink":"http://AlexAndSchaefer.github.io/publication/parcellation_biorxiv/","publishdate":"2017-06-06T00:00:00Z","relpermalink":"/publication/parcellation_biorxiv/","section":"publication","summary":"A central goal in systems neuroscience is the parcellation of the cerebral cortex into discrete neurobiological “atoms”. Resting-state functional magnetic resonance imaging (rs-fMRI) offers the possibility of in-vivo human cortical parcellation. Almost all previous parcellations relied on one of two approaches. The local gradient approach detects abrupt transitions in functional connectivity patterns. These transitions potentially reflect cortical areal boundaries defined by histology or visuotopic fMRI. By contrast, the global similarity approach clusters similar functional connectivity patterns regardless of spatial proximity, resulting in parcels with homogeneous (similar) rs-fMRI signals. Here we propose a gradient-weighted Markov Random Field (gwMRF) model integrating local gradient and global similarity approaches. Using task-fMRI and rs-fMRI across diverse acquisition protocols, we found gwMRF parcellations to be more homogeneous than four previously published parcellations. Furthermore, gwMRF parcellations agreed with the boundaries of certain cortical areas defined using histology and visuotopic fMRI. Some parcels captured sub-areal (somatotopic and visuotopic) features that likely reflect distinct computational units within known cortical areas. These results suggest that gwMRF parcellations reveal neurobiologically meaningful features of brain organization and are potentially useful for future applications requiring dimensionality reduction of voxel-wise fMRI data. Multi-resolution parcellations generated from 1489 participants are available at FREESURFER_WIKI LINK_TO_BE_ADDED.","tags":null,"title":"Local-Global Parcellation of the Human Cerebral Cortex From Intrinsic Functional Connectivity MRI","type":"publication"},{"authors":null,"categories":null,"content":" Recently, I wanted to make a blog post on prediction problems I encounter in my work as data scientist at project-a. While working on the post I realized that feature encoding might be too basic for some readers; while very much needed for others. As a consequence I decided feature encoding should become its own post.\nWhat is feature encoding? Not all information we have at hand is suited for the machine learning algorithm we want to use. Therefore, we have to employ a mapping or encoding e.g. from text to numbers. Here, I want to explain some basice feature encoding and give examples in python.\nTwo Types of Features Usually you encounter two types of features: numerical or categorical.\n Numerical features are usually real or integer numbers. Example numerical features are revenue of a customer, days since last order or number of orders. Categorical features are often given as text or boolean variables. Examples for categorical features are first product category, first marketing channel or order delivery type.  While numerical features are often straight forward to integrate, categorical features need a bit more work. Most machine learning algorithms do not understand text so we need to encode the text into numbers.\n   Categorical Feature Encoded Feature     \u0026quot;Text A\u0026quot; 0   \u0026quot;Text B\u0026quot; 1   \u0026quot;Text C\u0026quot; 2    Training the encoder with sklearn can be as simple as this:\nfrom sklearn.preprocessing import LabelEncoder le = LabelEncoder() le.fit(list(data.values.flatten()) )  And then applying the label encoder onto the data\nle.transform(list(data.values))  This ensures a one to one mapping and makes the categorical features understandable for our ML algorithm. However, a numbering like $$0,1,2, \\dotsc $$ implies an ordering e.g. $$ 0 \u0026lt; 1 \u0026lt; 2 \u0026lt; \\dotsc .$$ Which also implies that $$ \u0026ldquo; \\mathrm{Text \\enspace A}\u0026rdquo; \u0026lt; \u0026ldquo;\\mathrm{Text \\enspace B}\u0026rdquo; \u0026lt; \u0026ldquo;\\mathrm{Text \\enspace C}\u0026rdquo; \u0026lt; \\dotsc ,$$ which we do not know and therefore not want to imply.\nThere is a solution to this problem which is a binary endcoding also called one hot encoding.\n   Categorical Feature Encoded Feature One Hot Encoded Feature     \u0026quot;Text A\u0026quot; 0 (1,0,0,\u0026hellip;)   \u0026quot;Text B\u0026quot; 1 (0,1,0,\u0026hellip;)   \u0026quot;Text C\u0026quot; 2 (0,0,1,\u0026hellip;)    In python this can look like this\nfrom sklearn.preprocessing import OneHotEncoder enc = OneHotEncoder(sparse=False,n_values=np.max(data)+1) enc.fit(data)  So let us assume you have $n$ different categories you will create $n$-dimensional binary feature vectors. In each dimension the feature do not create an ordering and only contain the information of beeing present or not. This also came at a price as you created $n-1$ new feature dimensions.\nProduction Setting In a production setting you have to keep training and testing as separated processes. Often training is very expensive (in terms of computing time) so you want to do it as few times as possible. However, you have to use the same encoding in training and testing process. As as a result you may want to save the encoder you used in the training setting, and load this encoder in the testing setting using joblib\nfrom sklearn.externals import joblib from sklearn.preprocessing import LabelEncoder if train : ## fit and write encoder le = LabelEncoder() le.fit(list(data.values.flatten()) ) # we want one encoder for everything joblib.dump(le, '../model/label_encoder.pkl') else : ## read encoder le = joblib.load('../model/label_encoder.pkl')  This scheme enforces a strict separation of train and test data. Which is a quality in itself I do often prefer. However, if your test data encloses a feature which was not present in your training this approach will fail. This cannot be fixed by fitting a new encoder without re-running the training step. Soutions might range from ignoring unknown features to constructing complex ETL processes.\n","date":1492077600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492077600,"objectID":"b9db74fb00751c0d27b907ca7cb43b66","permalink":"http://AlexAndSchaefer.github.io/post/featureencoding/","publishdate":"2017-04-13T10:00:00Z","relpermalink":"/post/featureencoding/","section":"post","summary":"Recently, I wanted to make a blog post on prediction problems I encounter in my work as data scientist at project-a. While working on the post I realized that feature encoding might be too basic for some readers; while very much needed for others. As a consequence I decided feature encoding should become its own post.\nWhat is feature encoding? Not all information we have at hand is suited for the machine learning algorithm we want to use.","tags":[],"title":"Basic Feature Encoding","type":"post"},{"authors":["Alexander Schaefer, R. Kong, B.T. Thomas Yeo"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1470700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470700800,"objectID":"dec0e2023f387a82f293fb01a9f7a823","permalink":"http://AlexAndSchaefer.github.io/publication/parcellation_human_brain/","publishdate":"2016-08-09T00:00:00Z","relpermalink":"/publication/parcellation_human_brain/","section":"publication","summary":"Brain disorders are seen as one of the greatest threats to public health in the 21st century. To develop new treatments we need a fundamental understanding of brain organization and function. Parcellation of the human brain is a central key for understanding complex human behavior and also a major challenge in systems neuroscience. Machine learning has become a central element in deriving human brain parcellations. Here, we give an overview of machine learning approaches to functional connectivity parcellation of the human brain with a special emphasis on mixture models and Markov random fields.","tags":null,"title":"Functional connectivity parcellation of the human brain","type":"publication"},{"authors":["Alexander Schaefer, Daniel S. Margulies, Gabriele Lohmann, Krzysztof J. Gorgolewski, Jonathan Smallwood, Stefan J. Kiebel, Arno Villringer"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1399334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1399334400,"objectID":"43487b0eb9ba079cc5336ff1c8737d0f","permalink":"http://AlexAndSchaefer.github.io/publication/dynamic_hubs/","publishdate":"2014-05-06T00:00:00Z","relpermalink":"/publication/dynamic_hubs/","section":"publication","summary":"Network studies of large-scale brain connectivity have demonstrated that highly connected areas, or ‘hubs’, are a key feature of human functional and structural brain organization. We use resting-state functional MRI data and connectivity clustering to identify multi network hubs and show that while hubs can belong to multiple networks their degree of integration into these different networks varies dynamically over time. In addition, we found that these network dynamics were inversely related to positive self-generated thoughts reported by individuals and were further decreased with older age. Moreover, the left caudate varied its degree of participation between a default mode subnetwork and a limbic network. This variation was predictive of individual differences in the reports of past-related thoughts. These results support an association between ongoing thought processes and network dynamics and offer a new approach to investigate the brain dynamics underlying mental experience.","tags":null,"title":"Dynamic network participation of functional connectivity hubs assessed by resting-state fMRI","type":"publication"},{"authors":["Alexander Schaefer, Eva M Quinque, Judy A Kipping, Katrin Arélin, Elisabeth Roggenhofer,Stefan Frisch, Arno Villringer, Karsten Mueller, Matthias L Schroeter"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1398816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398816000,"objectID":"00971ab8d7eac7a7a122ea72b9702fc0","permalink":"http://AlexAndSchaefer.github.io/publication/early_small_vessel/","publishdate":"2014-04-30T00:00:00Z","relpermalink":"/publication/early_small_vessel/","section":"publication","summary":"Cerebral small vessel disease, mainly characterized by white matter lesions and lacunes, has a high clinical impact as it leads to vascular dementia. Recent studies have shown that this disease impairs frontoparietal networks. Here, we apply resting-state magnetic resonance imaging and data-driven whole-brain imaging analysis methods (eigenvector centrality) to investigate changes of the functional connectome in early small vessel disease. We show reduced connectivity in frontoparietal networks, whereas connectivity increases in the cerebellum. These functional changes are closely related to white matter lesions and typical neuropsychological deficits associated with small vessel disease.","tags":null,"title":"Early small vessel disease affects frontoparietal and cerebellar hubs in close correlation with clinical symptoms—a resting-state fMRI study","type":"publication"}]